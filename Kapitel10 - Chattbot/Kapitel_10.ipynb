{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cabcbf47",
   "metadata": {},
   "source": [
    "1. Skapa en chattbot som svarar på frågor utifrån något dokument som du själv väljer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7778472",
   "metadata": {},
   "source": [
    "1. 0 IMPORTER + API‑KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318b8d6",
   "metadata": {},
   "source": [
    "I detta steg importerar vi alla bibliotek som behövs för chatboten och aktiverar API‑nyckeln till Groq. Importerna gör att vi kan läsa PDF‑dokument, skapa text‑embeddings och skicka frågor till språkmodellen. Detta är grunden som resten av chatboten bygger på, och utan dessa delar kan vi varken läsa dokumentet eller generera svar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f2c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# IMPORTER\n",
    "# ---------------------------------------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from groq import Groq\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# API-KEY (GROQ)\n",
    "# ---------------------------------------------------------\n",
    "api_key = \"\"\n",
    "client = Groq(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99ba849",
   "metadata": {},
   "source": [
    "1. 1 LÄS IN DOKUMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599e833",
   "metadata": {},
   "source": [
    "I detta steg läser vi in PDF‑filerna som chatboten ska använda som kunskapskälla. Koden öppnar varje dokument, går igenom alla sidor och plockar ut texten. All text samlas i en enda stor sträng som senare används för att skapa embeddings och svara på frågor. Detta gör att chatboten kan söka i innehållet och ge svar baserat på dokumentens faktiska text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c248f0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totala antal tecken: 61502\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 1. LÄS IN DOKUMENT (TVÅ PDF-FILER)\n",
    "# ---------------------------------------------------------\n",
    "paths = [\n",
    "    \"6.1 AF TOJOS PLAST.pdf\",\n",
    "    \"6.2.1 MARK BYGG RAMBESKRIVNNG.pdf\"\n",
    "]\n",
    "\n",
    "text = \"\"\n",
    "for p in paths:\n",
    "    reader = PdfReader(p)\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "print(f\"Totala antal tecken: {len(text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5479baaa",
   "metadata": {},
   "source": [
    "1. 2 CHUNKING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8e1d06",
   "metadata": {},
   "source": [
    "Här ska vi dela upp den långa texten från PDF‑filerna i mindre bitar, så kallade chunks. Det gör vi för att modellen ska kunna söka och jämföra text på ett effektivt sätt. Varje chunk får en bestämd längd med lite överlapp, så att viktig information inte hamnar mitt emellan två delar. Dessa chunks används senare när vi skapar embeddings och bygger själva frågesystemet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05b81c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antal chunks: 42\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 2. CHUNKING\n",
    "# ---------------------------------------------------------\n",
    "chunks = []\n",
    "n = 1800\n",
    "overlap = 300\n",
    "\n",
    "for i in range(0, len(text), n - overlap):\n",
    "    chunks.append(text[i:i + n])\n",
    "\n",
    "print(f\"Antal chunks: {len(chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96969698",
   "metadata": {},
   "source": [
    "1. 3 EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc8bb78",
   "metadata": {},
   "source": [
    "I detta steg skapar vi embeddings för alla textbitar som vi gjorde i chunking‑steget. Vi laddar en färdig modell som omvandlar text till numeriska vektorer. Dessa vektorer gör det möjligt för chatboten att jämföra frågor med innehållet i dokumentet och hitta den mest relevanta texten. Embeddings är alltså grunden för att chatboten ska kunna förstå och söka i texten på ett smart sätt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb80ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 3. EMBEDDINGS (LOCAL SENTENCE-TRANSFORMER)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def create_embedding(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# Skapa embeddings för alla chunks\n",
    "chunk_embeddings = [create_embedding(chunk) for chunk in chunks]\n",
    "print(f\"Embeddings skapade: {len(chunk_embeddings)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1730cc8",
   "metadata": {},
   "source": [
    "1. 4 SEMANTISK SÖKNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ab347",
   "metadata": {},
   "source": [
    "Här bygger vi funktionen (semantiska sökningen) som chatboten använder för att hitta relevant text i dokumentet. Först räknar vi ut likheten mellan frågans embedding och varje chunk med hjälp av cosinuslikhet. Sedan sorterar vi resultaten och plockar ut de textdelar som liknar frågan mest. Dessa delar skickas vidare till modellen så att chatboten kan svara utifrån rätt innehåll."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a538cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 4. SEMANTISK SÖKNING\n",
    "# ---------------------------------------------------------\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def semantic_search(query, chunks, embeddings, k=5):\n",
    "    query_embedding = create_embedding(query)\n",
    "    scores = []\n",
    "\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        score = cosine_similarity(query_embedding, emb)\n",
    "        scores.append((i, score))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_indices = [i for i, _ in scores[:k]]\n",
    "    return [chunks[i] for i in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c89f781",
   "metadata": {},
   "source": [
    "1. 5 SYSTEM PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d6f71",
   "metadata": {},
   "source": [
    "Här definierar vi systemprompten som styr hur modellen ska bete sig när den svarar. Jag kommer ställa dig en fråga, och jag vill att du svarar baserat bara på kontexten jag skickar med, och ingen annan information. Om det inte finns nog med information i kontexten för att svara på frågan, säg \"Det vet jag inte\". Försök inte att gissa. Formulera dig enkelt och dela upp svaret i fina stycken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6443b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 5. SYSTEM PROMPT\n",
    "# ---------------------------------------------------------\n",
    "system_prompt = \"\"\"Jag kommer ställa dig en fråga, och jag vill att du svarar\n",
    "baserat bara på kontexten jag skickar med, och ingen annan information.\n",
    "Om det inte finns nog med information i kontexten för att svara på frågan,\n",
    "säg \"Det vet jag inte\". Försök inte att gissa.\n",
    "Formulera dig enkelt och dela upp svaret i fina stycken.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cfcc94",
   "metadata": {},
   "source": [
    "1. 6 GENERERA USER PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf02b131",
   "metadata": {},
   "source": [
    "Här skapar vi user‑prompten genom att först hämta relevanta chunks med semantisk sökning och sedan bygga en enkel text som innehåller både frågan och kontexten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a14bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 6. GENERERA USER PROMPT\n",
    "# ---------------------------------------------------------\n",
    "def generate_user_prompt(query):\n",
    "    context = \"\\n\".join(semantic_search(query, chunks, chunk_embeddings))\n",
    "    return f\"Frågan är: {query}\\n\\nHär är kontexten:\\n{context}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585fef5f",
   "metadata": {},
   "source": [
    "1. 7 GENERERA SVAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61049c92",
   "metadata": {},
   "source": [
    "Här genererar vi själva svaret genom att bygga user‑prompten, skicka både system‑ och user‑meddelandet till modellen och sedan hämta ut texten från modellens första svarsalternativ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d3467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 7. GENERERA SVAR (GROQ CHAT)\n",
    "# ---------------------------------------------------------\n",
    "def generate_response(system_prompt, user_message, model=\"llama-3.1-8b-instant\"):\n",
    "    full_prompt = generate_user_prompt(user_message)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": full_prompt}\n",
    "        ]\n",
    "    )\n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d7049",
   "metadata": {},
   "source": [
    "1. 8 CHATT‑LOOP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549df3e3",
   "metadata": {},
   "source": [
    "Här kör vi själva chattloopen som tar emot användarens fråga, skickar den till modellen via generate_response och skriver ut modellens svar tills användaren avslutar med “exit”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3411b7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 8. CHATT-LOOP\n",
    "# ---------------------------------------------------------\n",
    "print(\"\\n*** Groq RAG chat ***\")\n",
    "print(\"Skriv <Exit> för att avsluta.\\n\")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"User: \")\n",
    "    if prompt.lower() == \"exit\":\n",
    "        break\n",
    "    else:\n",
    "        answer = generate_response(system_prompt, prompt)\n",
    "        print(\"Groq:\", answer, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05caa5e",
   "metadata": {},
   "source": [
    "1. 9 VECTOR STORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7c720b",
   "metadata": {},
   "source": [
    "Här skapar vi en vector store som sparar varje chunk tillsammans med dess embedding och metadata i en parquet‑fil, så att vi kan ladda allt direkt vid nästa körning utan att behöva läsa PDF:erna eller räkna om embeddings igen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba370cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 9. VECTOR STORE (för att spara embeddings)\n",
    "# ---------------------------------------------------------\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def save(self, filename=\"embeddings.parquet\"):\n",
    "        df = pl.DataFrame({\n",
    "            \"vectors\": self.vectors,\n",
    "            \"texts\": self.texts,\n",
    "            \"metadata\": self.metadata\n",
    "        })\n",
    "        df.write_parquet(filename)\n",
    "        print(f\"Embeddings sparade till {filename}\")\n",
    "    \n",
    "    def load(self, filename=\"embeddings.parquet\"):\n",
    "        df = pl.read_parquet(filename)\n",
    "        self.vectors = df[\"vectors\"].to_list()\n",
    "        self.texts = df[\"texts\"].to_list()\n",
    "        self.metadata = df[\"metadata\"].to_list()\n",
    "        print(f\"Embeddings laddade från {filename}\")\n",
    "\n",
    "vector_store = VectorStore()\n",
    "for i, chunk in enumerate(chunks):\n",
    "    vector_store.add_item(chunk, chunk_embeddings[i], {\"index\": i})\n",
    "vector_store.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79e95a9",
   "metadata": {},
   "source": [
    "1. 10 EVALUERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58980195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 10. EVALUERING\n",
    "# ---------------------------------------------------------\n",
    "validation_data = [\n",
    "    {\n",
    "        \"question\": \"Vad handlar dokumenten om?\",\n",
    "        \"ideal_answer\": \"Dokument om byggprojekt och plastmaterial.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluation_system_prompt = \"\"\"Du är ett utvärderingssystem.\n",
    "Poäng: 1 = korrekt, 0.5 = delvis, 0 = fel.\n",
    "Motivera kort.\"\"\"\n",
    "\n",
    "query = validation_data[0][\"question\"]\n",
    "ai_answer = generate_response(system_prompt, query)\n",
    "\n",
    "evaluation_prompt = f\"\"\"Fråga: {query}\n",
    "AI-svar: {ai_answer}\n",
    "Ideal-svar: {validation_data[0]['ideal_answer']}\"\"\"\n",
    "\n",
    "evaluation = generate_response(evaluation_system_prompt, evaluation_prompt)\n",
    "print(\"\\nUtvärdering:\")\n",
    "print(evaluation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
